{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hours Studied</th>\n",
       "      <th>Previous Scores</th>\n",
       "      <th>Sleep Hours</th>\n",
       "      <th>Sample Question Papers Practiced</th>\n",
       "      <th>Performance Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>99</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>75</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hours Studied  Previous Scores  Sleep Hours  \\\n",
       "0              7               99            9   \n",
       "1              4               82            4   \n",
       "2              8               51            7   \n",
       "3              5               52            5   \n",
       "4              7               75            8   \n",
       "\n",
       "   Sample Question Papers Practiced  Performance Index  \n",
       "0                                 1                 91  \n",
       "1                                 2                 65  \n",
       "2                                 2                 45  \n",
       "3                                 2                 36  \n",
       "4                                 5                 66  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Student_performance.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7, 99,  9,  1],\n",
       "       [ 4, 82,  4,  2],\n",
       "       [ 8, 51,  7,  2],\n",
       "       ...,\n",
       "       [ 3, 60,  6,  1],\n",
       "       [ 7, 66,  8,  7],\n",
       "       [ 3, 66,  4,  6]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:,0:4].values\n",
    "y = df.iloc[:,-1].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 91,  65,  45,  36,  66,  61,  63,  42,  61,  69,  84,  73,  27,\n",
       "        33,  68,  43,  67,  70,  30,  63,  71,  85,  73,  57,  35,  49,\n",
       "        66,  83,  74,  74,  39,  36,  58,  47,  60,  74,  42,  68,  32,\n",
       "        64,  45,  39,  58,  36,  71,  54,  17,  54,  58,  53,  27,  65,\n",
       "        75,  52,  78,  91,  33,  47,  78,  38,  70,  98,  87,  49,  41,\n",
       "        71,  54,  42,  91,  61,  74,  54,  81,  52,  65,  36,  61,  35,\n",
       "        15,  88,  45,  49,  33,  60,  71,  81,  67,  95,  58,  29,  21,\n",
       "        38,  60,  76,  69,  30,  57,  81,  36,  25,  27,  61,  34,  76,\n",
       "        57,  45,  76,  83,  50,  81,  66,  38,  56,  25,  56,  82,  23,\n",
       "        56,  46,  43,  30,  92,  45,  70,  36,  71,  49,  82,  71,  43,\n",
       "        77,  86,  34,  49,  69,  84,  41,  68,  44,  41,  58,  68,  94,\n",
       "        47,  40, 100,  23,  36,  47,  60,  83,  33,  36,  74,  42,  47,\n",
       "        31,  26,  58,  42,  18,  85,  36,  58,  45,  60,  56,  42,  33,\n",
       "        77,  51,  72,  57,  53,  16,  45,  49,  67,  49,  73,  65,  27,\n",
       "        72,  74,  38,  67,  28,  73,  72,  42,  47,  77,  49,  30,  54,\n",
       "        56,  75,  78,  32,  89,  29,  57,  48,  27,  38,  18,  66,  65,\n",
       "        27,  33,  37,  29,  26,  72,  77,  45,  41,  73,  43,  39,  54,\n",
       "        62,  63,  59,  46,  42,  30,  63,  89,  62,  77,  88,  35,  60,\n",
       "        92,  38,  56,  70,  35,  47,  66,  43,  81,  43,  46,  61,  27,\n",
       "        77,  43,  60,  48,  47,  60,  53,  19,  23,  81,  41,  71,  75,\n",
       "        89,  28,  60,  54,  41,  52,  53,  52,  47,  71,  42,  21,  48,\n",
       "        64,  38,  49,  35,  73,  81,  47,  36,  63,  52,  58,  77,  58,\n",
       "        33,  67,  76,  62,  37,  86,  37,  88,  38,  76,  57,  82,  35,\n",
       "        41,  75,  51,  34,  92,  39,  56,  69,  69,  40,  63,  86,  89,\n",
       "        94,  79,  44,  33,  36,  50,  94,  22,  44,  60,  64,  58,  78,\n",
       "        51,  30,  29,  22,  51,  45,  70,  10,  38,  30,  82,  74,  87,\n",
       "        77,  72,  51,  40,  72,  68,  22,  67,  18,  64,  66,  50,  68,\n",
       "        44,  90,  41,  83,  57,  64,  18,  62,  66,  85,  62,  62,  62,\n",
       "        26,  66,  43,  30,  25,  77,  88,  62,  82,  73,  44,  40,  39,\n",
       "        34,  27,  73,  46,  78,  80,  33,  68,  45,  56,  58,  37,  88,\n",
       "        75,  70,  62,  57,  32,  53,  82,  41,  75,  63,  17,  24,  69,\n",
       "        47,  43,  48,  17,  75,  54,  59,  56,  20,  34,  54,  83,  59,\n",
       "        63,  72,  70,  41,  54,  21,  83,  64,  51,  66,  83,  26,  49,\n",
       "        85,  95,  56,  50,  23,  26,  30,  28,  48,  46,  73,  45,  76,\n",
       "        71,  60,  77,  36,  96,  62,  22,  21,  79,  82,  55,  22,  70,\n",
       "        37,  42,  64,  72,  42,  56,  43,  24,  55,  64,  34,  78,  71,\n",
       "        33,  46,  86,  34,  26,  44,  53,  61,  84,  40,  53,  67,  58,\n",
       "        41,  89,  85,  33,  20,  66,  38,  25,  43,  79,  16,  67,  80,\n",
       "        26,  57,  91,  29,  48,  84,  33,  82,  81,  24,  81,  54,  25,\n",
       "        53,  48,  40,  66,  29,  73,  89,  27,  40,  83,  69,  72,  77,\n",
       "        32,  49,  44,  57,  34,  76,  22,  30,  67,  71,  29,  74,  67,\n",
       "        46,  34,  76,  64,  46,  68,  58,  41,  64,  84,  70,  77,  60,\n",
       "        65,  21,  60,  48,  77,  63,  68,  81,  29,  66,  36,  29,  32,\n",
       "        31,  45,  65,  57,  58,  38,  22,  48,  59,  58,  62,  30,  56,\n",
       "        40,  59,  65,  43,  48,  24,  51,  81,  62,  43,  63,  73,  76,\n",
       "        43,  34,  57,  72,  47,  52,  49,  77,  41,  56,  89,  65,  49,\n",
       "        48,  47,  37,  48,  61,  77,  75,  57,  45,  82,  90,  42,  74,\n",
       "        60,  73,  40,  32,  67,  46,  65,  42,  69,  62,  75,  40,  43,\n",
       "        42,  84,  15,  39,  26,  73,  34,  34,  37,  34,  85,  31,  18,\n",
       "        92,  44,  72,  40,  31,  25,  44,  27,  66,  34,  70,  81,  56,\n",
       "        30,  60,  64,  74,  52,  84,  66,  32,  47,  51,  67,  57,  71,\n",
       "        86,  77,  72,  24,  60,  55,  54,  39,  19,  68,  83,  72,  49,\n",
       "        74,  86,  26,  60,  54,  73,  61,  45,  57,  68,  72,  17,  37,\n",
       "        54,  22,  73,  47,  63,  63,  68,  52,  64,  38,  60,  45],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.07288754 -0.40348868  0.26362306  0.18384488]\n",
      " [-0.68995764  0.35259821 -1.48691742 -0.51890687]\n",
      " [ 0.07590218  0.11995609  1.43065005  1.58934837]\n",
      " ...\n",
      " [-1.07288754  1.45764828  1.43065005  0.88659662]\n",
      " [-1.07288754 -1.10141504  1.43065005  0.53522075]\n",
      " [ 1.2246919  -0.92693345  1.43065005  0.88659662]]\n",
      "[[-0.80781605 -0.305478    0.94258306  1.36112025]\n",
      " [-1.58003343  0.39299331  0.94258306 -0.15601774]\n",
      " [-0.03559867  0.5094052   0.33571451 -1.67315572]\n",
      " [ 1.1227274   1.03325868 -0.27115403  1.74040475]\n",
      " [ 1.50883609 -1.17856713  0.94258306 -1.29387123]\n",
      " [-1.19392474 -0.65471365 -0.87802257  1.36112025]\n",
      " [ 0.35051002 -1.35318496 -0.87802257 -0.91458673]\n",
      " [-1.58003343 -0.42188988  0.94258306 -0.53530223]\n",
      " [-0.03559867  0.62581708  0.33571451  1.36112025]\n",
      " [ 0.35051002  1.20787651  0.94258306 -1.67315572]\n",
      " [-0.42170736  1.09146462  0.94258306  0.98183575]\n",
      " [-0.03559867  1.49890622  0.33571451 -0.91458673]\n",
      " [ 0.73661871  0.62581708 -0.87802257 -1.67315572]\n",
      " [ 0.35051002  1.49890622 -0.87802257 -1.29387123]\n",
      " [-0.80781605 -1.4113909  -1.48489112 -0.15601774]\n",
      " [-0.03559867 -0.36368394  0.33571451 -0.91458673]\n",
      " [ 1.50883609 -0.59650771 -0.27115403 -0.15601774]\n",
      " [ 0.73661871 -1.4113909   1.5494516  -1.29387123]\n",
      " [-1.58003343  1.03325868 -0.87802257  1.74040475]\n",
      " [-0.03559867 -0.36368394  0.33571451 -0.15601774]\n",
      " [ 0.73661871 -0.94574336 -0.27115403 -0.53530223]\n",
      " [ 0.35051002  1.78993593 -1.48489112  0.98183575]\n",
      " [-1.19392474 -0.94574336  0.33571451  1.36112025]\n",
      " [ 0.35051002 -1.46959684  1.5494516   0.60255126]\n",
      " [ 0.35051002  0.39299331 -1.48489112  0.60255126]\n",
      " [-0.03559867 -1.06215525 -1.48489112 -0.53530223]\n",
      " [-1.58003343  0.74222897 -0.27115403 -1.29387123]\n",
      " [-0.80781605  0.21837549  0.94258306 -0.53530223]\n",
      " [-0.42170736  0.80043491 -1.48489112 -0.91458673]\n",
      " [ 0.35051002  0.5094052  -0.27115403 -0.15601774]\n",
      " [-1.58003343  0.62581708  1.5494516  -0.15601774]\n",
      " [ 1.1227274  -1.64421467  0.33571451  0.98183575]\n",
      " [ 1.1227274   1.03325868  1.5494516  -0.91458673]\n",
      " [-0.42170736  0.74222897 -0.87802257 -0.91458673]\n",
      " [ 0.35051002  0.04375766 -0.87802257  0.60255126]\n",
      " [-1.19392474 -0.77112553  1.5494516  -0.15601774]\n",
      " [-0.42170736  1.6153181   0.33571451  1.36112025]\n",
      " [ 1.1227274  -1.46959684  0.33571451 -0.53530223]\n",
      " [ 0.73661871  0.97505274  1.5494516  -0.15601774]\n",
      " [-0.03559867  0.21837549 -0.27115403 -1.29387123]\n",
      " [ 1.50883609  0.85864085 -1.48489112 -1.29387123]\n",
      " [ 0.35051002  0.27658143 -0.87802257  0.60255126]\n",
      " [ 0.73661871 -1.17856713 -1.48489112 -1.29387123]\n",
      " [ 1.1227274   1.44070028 -0.27115403  1.74040475]\n",
      " [-1.58003343 -0.65471365  1.5494516  -0.91458673]\n",
      " [-1.58003343 -0.94574336 -0.87802257 -0.91458673]\n",
      " [ 1.50883609  0.91684679 -0.27115403  0.60255126]\n",
      " [ 1.50883609  0.21837549 -0.87802257  0.60255126]\n",
      " [-0.42170736  1.78993593  0.94258306  0.22326676]\n",
      " [-1.19392474 -0.07265423  0.33571451  1.36112025]\n",
      " [-0.80781605 -1.29497902 -1.48489112 -0.91458673]\n",
      " [ 1.1227274   1.6153181   0.94258306  1.74040475]\n",
      " [ 0.35051002 -1.52780279 -1.48489112  0.60255126]\n",
      " [-0.42170736 -0.82933148  0.94258306 -0.15601774]\n",
      " [ 0.35051002 -0.53830177  0.33571451  0.22326676]\n",
      " [-0.80781605 -1.06215525 -0.87802257  0.22326676]\n",
      " [-1.58003343 -1.64421467  0.94258306  1.74040475]\n",
      " [-1.58003343  0.91684679  1.5494516   1.74040475]\n",
      " [-1.19392474 -0.36368394  1.5494516  -0.15601774]\n",
      " [-0.42170736 -1.17856713 -0.87802257 -1.29387123]\n",
      " [ 1.50883609  1.44070028  0.33571451  0.22326676]\n",
      " [-0.03559867 -0.71291959 -0.87802257 -0.91458673]\n",
      " [-1.58003343  1.78993593 -0.27115403  1.74040475]\n",
      " [-0.03559867  0.5094052   0.94258306 -0.91458673]\n",
      " [ 1.1227274   1.44070028  1.5494516   0.98183575]\n",
      " [-0.03559867 -0.65471365 -0.87802257 -0.91458673]\n",
      " [-0.42170736 -1.4113909   0.33571451  0.60255126]\n",
      " [ 1.1227274   0.56761114 -0.87802257  0.98183575]\n",
      " [ 1.1227274   0.33478737  0.94258306 -1.29387123]\n",
      " [-1.58003343  1.38249433  0.33571451  0.98183575]\n",
      " [ 1.50883609 -0.07265423 -1.48489112 -0.53530223]\n",
      " [-1.58003343  0.16016954  0.33571451  1.74040475]\n",
      " [-1.58003343 -0.77112553  0.94258306 -1.29387123]\n",
      " [ 1.1227274   1.32428839 -1.48489112  0.22326676]\n",
      " [ 1.50883609  1.49890622  1.5494516  -1.29387123]\n",
      " [ 1.50883609 -0.94574336 -1.48489112  0.22326676]\n",
      " [ 1.1227274   0.39299331 -1.48489112 -0.53530223]\n",
      " [-1.58003343  0.80043491  1.5494516  -0.15601774]\n",
      " [-0.03559867  0.85864085 -0.87802257 -1.67315572]\n",
      " [-0.80781605  1.55711216 -0.87802257 -1.29387123]\n",
      " [-1.19392474 -1.12036119 -0.87802257  0.98183575]\n",
      " [ 1.50883609  1.26608245 -0.87802257  1.36112025]\n",
      " [-0.03559867  0.39299331 -0.27115403  0.98183575]\n",
      " [ 0.73661871 -1.52780279 -0.87802257  0.98183575]\n",
      " [-1.58003343  0.45119926 -1.48489112 -0.91458673]\n",
      " [ 0.35051002 -1.46959684 -1.48489112 -1.67315572]\n",
      " [-1.58003343 -1.4113909   0.94258306  1.36112025]\n",
      " [ 0.73661871 -1.17856713  1.5494516   1.74040475]\n",
      " [-1.19392474 -1.12036119 -0.27115403 -0.15601774]\n",
      " [-1.19392474 -0.01444828  0.33571451 -1.29387123]\n",
      " [ 1.1227274   0.1019636   0.94258306 -0.53530223]\n",
      " [-0.42170736 -0.53830177  0.94258306 -0.15601774]\n",
      " [ 0.73661871  0.85864085 -0.87802257  0.60255126]\n",
      " [ 1.1227274  -0.42188988  0.33571451 -0.91458673]\n",
      " [-0.42170736  0.21837549  0.94258306 -1.67315572]\n",
      " [ 1.1227274  -0.65471365  0.94258306 -0.91458673]\n",
      " [-0.03559867  1.20787651 -0.87802257  0.60255126]\n",
      " [ 1.50883609  0.21837549  0.94258306 -0.91458673]\n",
      " [ 1.50883609 -0.71291959  0.94258306 -0.91458673]\n",
      " [-1.19392474  0.74222897 -1.48489112 -0.53530223]\n",
      " [-0.80781605  0.62581708  0.33571451  0.22326676]\n",
      " [ 1.50883609  0.21837549  0.33571451  0.60255126]\n",
      " [-0.42170736 -0.36368394 -1.48489112  0.60255126]\n",
      " [-1.19392474 -0.82933148 -1.48489112  1.74040475]\n",
      " [ 0.73661871 -1.35318496  0.33571451  0.22326676]\n",
      " [ 1.1227274  -0.01444828  0.33571451 -0.53530223]\n",
      " [-1.19392474 -0.24727205 -0.27115403  0.22326676]\n",
      " [-0.42170736  0.56761114  0.94258306 -0.53530223]\n",
      " [ 0.35051002  0.74222897  1.5494516   1.74040475]\n",
      " [-1.19392474 -0.18906611 -0.27115403 -0.91458673]\n",
      " [ 0.73661871 -0.59650771 -0.27115403 -0.91458673]\n",
      " [ 0.35051002  0.85864085 -1.48489112  0.98183575]\n",
      " [ 1.1227274   0.91684679  0.33571451  0.22326676]\n",
      " [-1.58003343 -1.52780279  0.94258306 -0.15601774]\n",
      " [ 1.1227274   1.49890622  0.94258306  0.60255126]\n",
      " [-0.03559867 -0.48009582 -1.48489112  0.22326676]\n",
      " [-0.42170736  1.32428839  1.5494516   0.22326676]\n",
      " [ 1.1227274  -1.35318496 -0.27115403 -0.91458673]\n",
      " [-1.19392474 -0.42188988 -0.27115403  0.60255126]\n",
      " [ 1.1227274  -0.82933148 -0.27115403  0.60255126]\n",
      " [ 1.1227274  -1.29497902  1.5494516   0.60255126]\n",
      " [ 0.35051002  1.73172999 -1.48489112 -0.15601774]\n",
      " [ 1.1227274  -1.4113909  -0.27115403  0.22326676]\n",
      " [-0.03559867 -0.59650771 -1.48489112  0.22326676]\n",
      " [ 1.1227274  -0.71291959 -0.87802257  0.60255126]\n",
      " [ 0.35051002 -0.94574336 -0.27115403  0.60255126]\n",
      " [-0.03559867 -1.64421467 -1.48489112  1.74040475]\n",
      " [-0.42170736 -1.35318496 -0.27115403  1.36112025]\n",
      " [ 0.35051002  1.55711216 -0.27115403  0.98183575]\n",
      " [ 0.73661871  0.16016954 -0.27115403 -1.29387123]\n",
      " [-0.03559867 -1.23677307  0.94258306  0.60255126]\n",
      " [-1.58003343  0.91684679  0.33571451 -0.91458673]\n",
      " [ 0.35051002  0.85864085  0.33571451 -0.91458673]\n",
      " [-0.03559867  1.55711216  1.5494516  -1.29387123]\n",
      " [-0.42170736  0.80043491  0.94258306 -0.15601774]\n",
      " [-1.58003343 -0.48009582  0.94258306 -0.53530223]\n",
      " [-0.42170736 -0.13086017  1.5494516  -0.53530223]\n",
      " [-0.03559867 -0.42188988 -0.27115403  0.60255126]\n",
      " [-0.42170736 -0.88753742 -0.27115403  1.74040475]\n",
      " [ 1.1227274  -0.07265423 -0.87802257 -1.29387123]\n",
      " [-0.80781605  0.68402303  0.33571451 -0.53530223]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "print(X_train)\n",
    "X_test = sc_X.fit_transform(X_test)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Initialize the classification models\n",
    "models = {\n",
    "    'Logistic Regression': make_pipeline(StandardScaler(), LogisticRegression(max_iter=200)),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'SVM': make_pipeline(StandardScaler(), SVC())\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "  k=5: 0.0571\n",
      "  k=7: 0.0613\n",
      "  k=10: 0.0556\n",
      "\n",
      "Model: Decision Tree\n",
      "  k=5: 0.1169\n",
      "  k=7: 0.1070\n",
      "  k=10: 0.1127\n",
      "\n",
      "Model: SVM\n",
      "  k=5: 0.0685\n",
      "  k=7: 0.0671\n",
      "  k=10: 0.0556\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define K values for cross-validation\n",
    "k_values = [5, 7, 10]\n",
    "\n",
    "# Perform cross-validation and print results\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    for k in k_values:\n",
    "        kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        scores = cross_val_score(model, X, y, cv=kf)\n",
    "        print(f\"  k={k}: {scores.mean():.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
